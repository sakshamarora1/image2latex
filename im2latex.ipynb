{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = \"/content/drive/MyDrive/latex/\"\n",
    "images_dir = \"formula_images_processed\"\n",
    "formula_list = \"im2latex_formulas.lst\"\n",
    "train_list = \"im2latex_train.lst\"\n",
    "validate_list = \"im2latex_validate.lst\"\n",
    "test_list = \"im2latex_test.lst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image2latexDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, formula_list, train_list):\n",
    "        self.images_dir = images_dir\n",
    "        # self.image_filenames = os.listdir(self.images_dir)\n",
    "\n",
    "        with open(formula_list, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\\n\") as f1:\n",
    "            self.formulas = [formula.replace(\"\\n\", \"\").replace(\"\\t\", \" \") for formula in f1.readlines()]\n",
    "        \n",
    "        with open(train_list, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\\n\") as f2:\n",
    "            self.train_set = [t.replace(\"\\n\", \"\").split() for t in f2.readlines()]\n",
    "    \n",
    "        # assert len(self.image_filenames) == len(self.formulas) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.train_set[idx]\n",
    "        formula = self.formulas[int(item[0])]\n",
    "        filename = item[1] + \".png\"\n",
    "        render_type = item[2]\n",
    "        image = PIL.Image.open(self.images_dir + \"/\" + filename)\n",
    "        return torchvision.transforms.ToTensor()(image), formula\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = image2latexDataset(images_dir, formula_list, train_list)\n",
    "validate_set = image2latexDataset(images_dir, formula_list, validate_list)\n",
    "test_set = image2latexDataset(images_dir, formula_list, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 'ds^{2} = (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}\\\\lbrace dr^2+r^2d\\\\theta^2+r^2sin^2\\\\theta d\\\\varphi^2\\\\rbrace -{dt^2\\\\over  (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}}\\\\, .\\\\label{eq:sps1}')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83884 9320 10355\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set), len(validate_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=32,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validate_set,\n",
    "    batch_size=32,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " ('ds^{2} = (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}\\\\lbrace dr^2+r^2d\\\\theta^2+r^2sin^2\\\\theta d\\\\varphi^2\\\\rbrace -{dt^2\\\\over  (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}}\\\\, .\\\\label{eq:sps1}',\n",
       "  '\\\\widetilde\\\\gamma_{\\\\rm hopf}\\\\simeq\\\\sum_{n>0}\\\\widetilde{G}_n{(-a)^n\\\\over2^{2n-1}}\\\\label{H4}',\n",
       "  '({\\\\cal L}_a g)_{ij} = 0, \\\\ \\\\ \\\\ \\\\ ({\\\\cal L}_a H)_{ijk} = 0 ,',\n",
       "  'S_{stat} = 2\\\\pi \\\\sqrt{N_5^{(1)} N_5^{(2)} N_5^{(3)}} \\\\left(\\\\sqrt{n} +\\\\sqrt{\\\\bar{n}}\\\\right)\\\\label{74}',\n",
       "  '\\\\hat N_3 = \\\\sum\\\\sp f_{j=1}a_j\\\\sp {\\\\dagger} a_j \\\\,. \\\\label{c5}',\n",
       "  '+ \\\\int\\\\!\\\\!d^D\\\\!z_1 d^D\\\\!z_2 d^D\\\\!z_3 \\\\left.       \\\\frac{\\\\delta^2 W}{\\\\delta j(x) \\\\delta j(z_1)} \\\\,       \\\\frac{\\\\delta^2 W}{\\\\delta j(x) \\\\delta j(z_2)} \\\\,       \\\\frac{\\\\delta^2 W}{\\\\delta j(x) \\\\delta j(z_3)} \\\\,       \\\\frac{\\\\delta^3 \\\\Gamma}                  {\\\\delta \\\\Phi(z_1) \\\\delta \\\\Phi(z_2) \\\\delta \\\\Phi(z_3)}          \\\\right] ,',\n",
       "  '\\\\,^{*}d\\\\,^{*}H=\\\\kappa \\\\,^{*}d\\\\phi = J_B  . \\\\label{bfm19}',\n",
       "  \"{\\\\phi''\\\\over A} +{1\\\\over A}\\\\left( -{1\\\\over 2}{A'\\\\over A}+2{B'\\\\over B}+{2\\\\over r}\\\\right)\\\\phi'-{2 \\\\over r^2}\\\\phi -\\\\lambda\\\\phi (\\\\phi^2-\\\\eta^2) =0\\\\,.\\\\label{eq=phi}\",\n",
       "  '\\\\label{maxw}\\\\partial_{\\\\mu} (F^{\\\\mu\\\\nu}-ej^{\\\\mu}x^{\\\\nu})=0 .',\n",
       "  'V_{ns}({\\\\tilde x})= \\\\left(\\\\frac{{\\\\tilde m}N^2}{16\\\\pi}\\\\right)N g^{2ns-1}{\\\\tilde x}^2 \\\\left\\\\{{\\\\tilde x}^2 -\\\\frac{2{\\\\tilde b}}{3}{\\\\tilde x}+\\\\frac{{\\\\tilde b}^2}{3} -(-1)^{ns}{\\\\tilde c}\\\\right\\\\} \\\\,.',\n",
       "  'g_{ij}(x)={1\\\\over a^2}\\\\,\\\\delta_{ij},~~\\\\phi^a(x)=\\\\phi^a,\\\\quad (a,\\\\phi^a\\\\!:~{\\\\rm const.}) \\\\label{finite-perturbation}',\n",
       "  '\\\\rho_L (q) = \\\\sum_{m=1}^L \\\\ P_L (m) \\\\ {1 \\\\over q^{m-1}}  \\\\ \\\\ .\\\\label{relat}',\n",
       "  '\\\\label{Elliott}exp\\\\left( -\\\\frac{\\\\partial}{\\\\partial \\\\alpha_{j}}\\\\theta^{jk}\\\\frac{\\\\partial}{\\\\partial \\\\alpha_{k}} \\\\right)',\n",
       "  'L_{0} = \\\\Phi(w) = \\\\bigtriangleup\\\\Phi(w) ,',\n",
       "  '\\\\left( D^{*}D^{*}+m^2\\\\right) {\\\\cal H}=0  \\\\label{3.3}',\n",
       "  '{dV\\\\over d\\\\Phi}= -{w\\\\Phi\\\\over \\\\Phi_{\\\\!_0}^2}\\\\, .\\\\label{w}',\n",
       "  '\\\\label{g=u,x}g(z,\\\\bar z)=-\\\\frac{1}{2}\\\\left[x(z,\\\\bar z)\\\\,s+x^*(z,\\\\bar z)\\\\,s^*+u^*(z,\\\\bar z)\\\\,t+u(z,\\\\bar z)\\\\,t^*\\\\right],',\n",
       "  'x^{c}_{\\\\mu}=x_{\\\\mu}+A_{\\\\mu}.\\\\label{transf}',\n",
       "  's = {S \\\\over V} = {A_H \\\\over l_p^8 V } = {T^2 \\\\over \\\\gamma}. \\\\label{entropy}',\n",
       "  \"\\\\psi(\\\\gamma) = \\\\exp{ -({\\\\textstyle{g^2 \\\\over2}}) \\\\int_{\\\\gamma} dy^a\\\\int_{\\\\gamma} dy^{a'} D_{1}(y-y') }\",\n",
       "  'E=E_0+\\\\frac{1}{2\\\\sinh(\\\\gamma(0)/2)}\\\\sinh\\\\left(\\\\gamma(0)\\\\left(\\\\frac{1}{2}+c(0)\\\\right)\\\\right)hc\\\\nu_{\\\\rm vib}',\n",
       "  '\\\\langle T_{zz}\\\\rangle =-3\\\\times \\\\frac{\\\\pi^2}{1440 a^4}.',\n",
       "  \"\\\\partial_{u} \\\\xi_{z}^{(1)} +{1\\\\over u} \\\\xi_{z}^{(1)}  = {1\\\\over (\\\\pi T R)^2 u}\\\\left[ C_z H_{zz}'  + C_t H_{tz}' \\\\right]\\\\,.\\\\label{gauge_eq_z_3_p}\",\n",
       "  '\\\\label{eq:action}  S\\\\sim\\\\tilde{\\\\psi} Q_o \\\\tilde{\\\\psi} +g_s^{1/2} \\\\tilde{\\\\psi}^3+\\\\tilde{\\\\phi} Q_c \\\\tilde{\\\\phi} + g_s \\\\tilde{\\\\phi}^3+ \\\\tilde{\\\\phi}B(g_s^{1/2}\\\\tilde{\\\\psi})+\\\\cdots.',\n",
       "  'C(x^{\\\\prime}, x^{\\\\prime\\\\prime}) = C \\\\Phi(x^{\\\\prime},x^{\\\\prime\\\\prime})\\\\ ,\\\\quad \\\\Phi(x^{\\\\prime}, x^{\\\\prime\\\\prime})=\\\\exp\\\\left[-ie\\\\int_{x^{\\\\prime\\\\prime}}^{x^{\\\\prime}} dx^{\\\\mu}A_{\\\\mu}(x)\\\\right]\\\\ , \\\\label{eq27}',\n",
       "  '\\\\label{atilde}\\\\tilde{\\\\alpha}=\\\\alpha \\\\beta^{-m}=\\\\left( \\\\begin{array}{ccc} \\\\omega_{k}^{-2y}\\\\omega_{2d}^{2m} &  0  & 0 \\\\\\\\0 & \\\\omega_{k}^{y}\\\\omega_{2d}^{-m} & 0  \\\\\\\\0 & 0 & \\\\omega_{k}^{y} \\\\omega_{2d}^{-m}  \\\\end{array} \\\\right)',\n",
       "  '\\\\label{req5}ds^2 = H^{-2} f(r) dt^2 + H^{2/(n-1)}(f(r)^{-1}dr^2 +r^2 d\\\\Omega_n^2),',\n",
       "  '%y^2 = \\\\rho \\\\; \\\\cosh \\\\beta \\\\; \\\\sin \\\\theta \\\\; \\\\sin \\\\phi\\\\qquad\\\\qquad y^3 = \\\\rho \\\\; \\\\cos \\\\theta\\\\label{eqn:1.7}%',\n",
       "  \"\\\\label{chi0'}e^A = e^{A_0} \\\\left( t_0 - {\\\\rm sign} (m) t  \\\\right)^{- \\\\frac{m}{2}} \\\\; , \\\\; \\\\; \\\\; \\\\; \\\\chi = \\\\chi_0 \\\\left( t_0 - {\\\\rm sign} (m) t \\\\right)^m \\\\; ,\",\n",
       "  '\\\\gamma_j{\\\\cal P}_{ji}= \\\\frac{4}{3}\\\\{[Ad\\\\,T][t^c_{8},[t^c_{8},{\\\\gamma}_j]][Ad\\\\,T^{-1}]\\\\}{Ad\\\\,{\\\\hat{g}}}_{ij}.',\n",
       "  'K_{\\\\mu\\\\nu}~=~\\\\frac{1}{2}\\\\dot{g}_{\\\\mu\\\\nu}.',\n",
       "  'X(u)= { {\\\\left( \\\\pm i + e^{3 \\\\eta} \\\\right)   \\\\left( -1 + {e^u} \\\\right)  \\\\left( 1 + {e^u} \\\\right) x_1 } \\\\over {2 {e^u} \\\\left( \\\\pm i + {e^{3 \\\\eta + u}} \\\\right) } },\\\\label{XII2}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def add_positional_features(tensor: torch.Tensor,\n",
    "                            min_timescale: float = 1.0,\n",
    "                            max_timescale: float = 1.0e4):\n",
    "    \"\"\"\n",
    "    Implements the frequency-based positional encoding described\n",
    "    in `Attention is all you Need\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : ``torch.Tensor``\n",
    "        a Tensor with shape (batch_size, timesteps, hidden_dim).\n",
    "    min_timescale : ``float``, optional (default = 1.0)\n",
    "        The largest timescale to use.\n",
    "    Returns\n",
    "    -------\n",
    "    The input tensor augmented with the sinusoidal frequencies.\n",
    "    \"\"\"\n",
    "    _, timesteps, hidden_dim = tensor.size()\n",
    "\n",
    "    timestep_range = get_range_vector(timesteps, tensor.device).data.float()\n",
    "    # We're generating both cos and sin frequencies,\n",
    "    # so half for each.\n",
    "    num_timescales = hidden_dim // 2\n",
    "    timescale_range = get_range_vector(\n",
    "        num_timescales, tensor.device).data.float()\n",
    "\n",
    "    log_timescale_increments = math.log(\n",
    "        float(max_timescale) / float(min_timescale)) / float(num_timescales - 1)\n",
    "    inverse_timescales = min_timescale * \\\n",
    "        torch.exp(timescale_range * -log_timescale_increments)\n",
    "\n",
    "    # Broadcasted multiplication - shape (timesteps, num_timescales)\n",
    "    scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)\n",
    "    # shape (timesteps, 2 * num_timescales)\n",
    "    sinusoids = torch.randn(\n",
    "        scaled_time.size(0), 2*scaled_time.size(1), device=tensor.device)\n",
    "    sinusoids[:, ::2] = torch.sin(scaled_time)\n",
    "    sinusoids[:, 1::2] = torch.sin(scaled_time)\n",
    "    if hidden_dim % 2 != 0:\n",
    "        # if the number of dimensions is odd, the cos and sin\n",
    "        # timescales had size (hidden_dim - 1) / 2, so we need\n",
    "        # to add a row of zeros to make up the difference.\n",
    "        sinusoids = torch.cat(\n",
    "            [sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)\n",
    "    return tensor + sinusoids.unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_range_vector(size: int, device) -> torch.Tensor:\n",
    "    return torch.arange(0, size, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "INIT = 1e-2\n",
    "\n",
    "class Image2LatexModel(nn.Module):\n",
    "    def __init__(self, out_size, emb_size, dec_rnn_h,\n",
    "                 enc_out_dim=512,  n_layer=1,\n",
    "                 add_pos_feat=False, dropout=0.):\n",
    "        \n",
    "        super(Image2LatexModel, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.rnn_decoder = nn.LSTMCell(dec_rnn_h+emb_size, dec_rnn_h)\n",
    "        self.embedding = nn.Embedding(out_size, emb_size)\n",
    "\n",
    "        self.init_wh = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wc = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wo = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.beta = nn.Parameter(torch.Tensor(enc_out_dim))\n",
    "        init.uniform_(self.beta, -INIT, INIT)\n",
    "        self.W_1 = nn.Linear(enc_out_dim, enc_out_dim, bias=False)\n",
    "        self.W_2 = nn.Linear(dec_rnn_h, enc_out_dim, bias=False)\n",
    "\n",
    "        self.W_3 = nn.Linear(dec_rnn_h+enc_out_dim, dec_rnn_h, bias=False)\n",
    "        self.W_out = nn.Linear(dec_rnn_h, out_size, bias=False)\n",
    "\n",
    "        self.add_pos_feat = add_pos_feat\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.uniform = Uniform(0, 1)\n",
    "\n",
    "    def forward(self, imgs, formulas, epsilon=1.):\n",
    "        \"\"\"args:\n",
    "        imgs: [B, C, H, W]\n",
    "        formulas: [B, MAX_LEN]\n",
    "        epsilon: probability of the current time step to\n",
    "                 use the true previous token\n",
    "        return:\n",
    "        logits: [B, MAX_LEN, VOCAB_SIZE]\n",
    "        \"\"\"\n",
    "        # encoding\n",
    "        encoded_imgs = self.encode(imgs)  # [B, H*W, 512]\n",
    "        # init decoder's states\n",
    "        dec_states, o_t = self.init_decoder(encoded_imgs)\n",
    "        max_len = formulas.size(1)\n",
    "        logits = []\n",
    "        for t in range(max_len):\n",
    "            tgt = formulas[:, t:t+1]\n",
    "            # schedule sampling\n",
    "            if logits and self.uniform.sample().item() > epsilon:\n",
    "                tgt = torch.argmax(torch.log(logits[-1]), dim=1, keepdim=True)\n",
    "            # ont step decoding\n",
    "            dec_states, O_t, logit = self.step_decoding(\n",
    "                dec_states, o_t, encoded_imgs, tgt)\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits, dim=1)  # [B, MAX_LEN, out_size]\n",
    "        return logits\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        encoded_imgs = self.cnn_encoder(imgs)  # [B, 512, H', W']\n",
    "        encoded_imgs = encoded_imgs.permute(0, 2, 3, 1)  # [B, H', W', 512]\n",
    "        B, H, W, _ = encoded_imgs.shape\n",
    "        encoded_imgs = encoded_imgs.contiguous().view(B, H*W, -1)\n",
    "        if self.add_pos_feat:\n",
    "            encoded_imgs = add_positional_features(encoded_imgs)\n",
    "        return encoded_imgs\n",
    "\n",
    "    def step_decoding(self, dec_states, o_t, enc_out, tgt):\n",
    "        \"\"\"Runing one step decoding\"\"\"\n",
    "\n",
    "        prev_y = self.embedding(tgt).squeeze(1)  # [B, emb_size]\n",
    "        inp = torch.cat([prev_y, o_t], dim=1)  # [B, emb_size+dec_rnn_h]\n",
    "        h_t, c_t = self.rnn_decoder(inp, dec_states)  # h_t:[B, dec_rnn_h]\n",
    "        h_t = self.dropout(h_t)\n",
    "        c_t = self.dropout(c_t)\n",
    "\n",
    "        # context_t : [B, C]\n",
    "        context_t, attn_scores = self._get_attn(enc_out, h_t)\n",
    "\n",
    "        # [B, dec_rnn_h]\n",
    "        o_t = self.W_3(torch.cat([h_t, context_t], dim=1)).tanh()\n",
    "        o_t = self.dropout(o_t)\n",
    "\n",
    "        # calculate logit\n",
    "        logit = F.softmax(self.W_out(o_t), dim=1)  # [B, out_size]\n",
    "\n",
    "        return (h_t, c_t), o_t, logit\n",
    "\n",
    "    def _get_attn(self, enc_out, h_t):\n",
    "        \"\"\"Attention mechanism\n",
    "        args:\n",
    "            enc_out: row encoder's output [B, L=H*W, C]\n",
    "            h_t: the current time step hidden state [B, dec_rnn_h]\n",
    "        return:\n",
    "            context: this time step context [B, C]\n",
    "            attn_scores: Attention scores\n",
    "        \"\"\"\n",
    "        # cal alpha\n",
    "        alpha = torch.tanh(self.W_1(enc_out)+self.W_2(h_t).unsqueeze(1))\n",
    "        alpha = torch.sum(self.beta*alpha, dim=-1)  # [B, L]\n",
    "        alpha = F.softmax(alpha, dim=-1)  # [B, L]\n",
    "\n",
    "        # cal context: [B, C]\n",
    "        context = torch.bmm(alpha.unsqueeze(1), enc_out)\n",
    "        context = context.squeeze(1)\n",
    "        return context, alpha\n",
    "\n",
    "    def init_decoder(self, enc_out):\n",
    "        \"\"\"args:\n",
    "            enc_out: the output of row encoder [B, H*W, C]\n",
    "          return:\n",
    "            h_0, c_0:  h_0 and c_0's shape: [B, dec_rnn_h]\n",
    "            init_O : the average of enc_out  [B, dec_rnn_h]\n",
    "            for decoder\n",
    "        \"\"\"\n",
    "        mean_enc_out = enc_out.mean(dim=1)\n",
    "        h = self._init_h(mean_enc_out)\n",
    "        c = self._init_c(mean_enc_out)\n",
    "        init_o = self._init_o(mean_enc_out)\n",
    "        return (h, c), init_o\n",
    "\n",
    "    def _init_h(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wh(mean_enc_out))\n",
    "\n",
    "    def _init_c(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wc(mean_enc_out))\n",
    "\n",
    "    def _init_o(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wo(mean_enc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Vocab File in  vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "\n",
    "START_TOKEN = 0\n",
    "PAD_TOKEN = 1\n",
    "END_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# buid sign2id\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.sign2id = {\"<s>\": START_TOKEN, \"</s>\": END_TOKEN,\n",
    "                        \"<pad>\": PAD_TOKEN, \"<unk>\": UNK_TOKEN}\n",
    "        self.id2sign = dict((idx, token)\n",
    "                            for token, idx in self.sign2id.items())\n",
    "        self.length = 4\n",
    "\n",
    "    def add_sign(self, sign):\n",
    "        if sign not in self.sign2id:\n",
    "            self.sign2id[sign] = self.length\n",
    "            self.id2sign[self.length] = sign\n",
    "            self.length += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def build_vocab(min_count=10):\n",
    "    \"\"\"\n",
    "    traverse training formulas to make vocab\n",
    "    and store the vocab in the file\n",
    "    \"\"\"\n",
    "    vocab = Vocab()\n",
    "    counter = Counter()\n",
    "\n",
    "    formulas_file = formula_list\n",
    "    with open(formula_list, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\\n\") as f1:\n",
    "        formulas = [formula.replace(\"\\n\", \"\").replace(\"\\t\", \" \") for formula in f1.readlines()]\n",
    "        \n",
    "    with open(train_list, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\\n\") as f2:\n",
    "        for line in f2:\n",
    "            idx, img_filename, render_type = line.strip('\\n').split()\n",
    "            idx = int(idx)\n",
    "            formula = formulas[idx].split()\n",
    "            counter.update(formula)\n",
    "\n",
    "    for word, count in counter.most_common():\n",
    "        if count >= min_count:\n",
    "            vocab.add_sign(word)\n",
    "    vocab_file = 'vocab.pkl'\n",
    "    print(\"Writing Vocab File in \", vocab_file)\n",
    "    with open(vocab_file, 'wb') as w:\n",
    "        pkl.dump(vocab, w)\n",
    "\n",
    "vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab including 3977 words!\n"
     ]
    }
   ],
   "source": [
    "def load_vocab():\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab = pkl.load(f)\n",
    "    print(\"Load vocab including {} words!\".format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "v = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.id2sign[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexProducer(object):\n",
    "    \"\"\"\n",
    "    Model wrapper, implementing batch greedy decoding and\n",
    "    batch beam search decoding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, vocab, beam_size=5, max_len=64, use_cuda=True):\n",
    "        \"\"\"args:\n",
    "            the path to model checkpoint\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self._sign2id = vocab.sign2id\n",
    "        self._id2sign = vocab.id2sign\n",
    "        self.max_len = max_len\n",
    "        self.beam_size = beam_size\n",
    "#         self._beam_search = BeamSearch(END_TOKEN, max_len, beam_size)\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"args:\n",
    "            imgs: images need to be decoded\n",
    "            beam_size: if equal to 1, use greedy decoding\n",
    "           returns:\n",
    "            formulas list of batch_size length\n",
    "        \"\"\"\n",
    "#         if self.beam_size == 1:\n",
    "        results = self._greedy_decoding(imgs)\n",
    "#         else:\n",
    "#             results = self._batch_beam_search(imgs)\n",
    "        return results\n",
    "\n",
    "    def _greedy_decoding(self, imgs):\n",
    "        imgs = imgs.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        enc_outs = self.model.encode(imgs)\n",
    "        dec_states, O_t = self.model.init_decoder(enc_outs)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        # storing decoding results\n",
    "        formulas_idx = torch.ones(\n",
    "            batch_size, self.max_len, device=self.device).long() * PAD_TOKEN\n",
    "        # first decoding step's input\n",
    "        tgt = torch.ones(\n",
    "            batch_size, 1, device=self.device).long() * START_TOKEN\n",
    "        with torch.no_grad():\n",
    "            for t in range(self.max_len):\n",
    "                dec_states, O_t, logit = self.model.step_decoding(\n",
    "                    dec_states, O_t, enc_outs, tgt)\n",
    "\n",
    "                tgt = torch.argmax(logit, dim=1, keepdim=True)\n",
    "                formulas_idx[:, t:t + 1] = tgt\n",
    "        results = self._idx2formulas(formulas_idx)\n",
    "        return results\n",
    "\n",
    "#     def _simple_beam_search_decoding(self, imgs):\n",
    "#         \"\"\"simpple beam search decoding (not support batch)\"\"\"\n",
    "#         self.model.eval()\n",
    "#         beam_results = [\n",
    "#             self._bs_decoding(img.unsqueeze(0))\n",
    "#             for img in imgs\n",
    "#         ]\n",
    "#         return beam_results\n",
    "\n",
    "    def _idx2formulas(self, formulas_idx):\n",
    "        \"\"\"convert formula id matrix to formulas list\"\"\"\n",
    "        results = []\n",
    "        for id_ in formulas_idx:\n",
    "            id_list = id_.tolist()\n",
    "            result = []\n",
    "            for sign_id in id_list:\n",
    "                if sign_id != END_TOKEN:\n",
    "                    result.append(self._id2sign[sign_id])\n",
    "                else:\n",
    "                    break\n",
    "            results.append(\" \".join(result))\n",
    "        return results\n",
    "\n",
    "#     def _bs_decoding(self, img):\n",
    "#         \"\"\"\n",
    "#         beam search decoding not support batch\n",
    "#         args:\n",
    "#             img: [1, C, H, W]\n",
    "#             beam_size: int\n",
    "#         return:\n",
    "#             formulas in str format\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         img = img.to(self.device)\n",
    "\n",
    "#         # encoding\n",
    "#         # img = img.unsqueeze(0)  # [1, C, H, W]\n",
    "#         enc_outs = self.model.encode(img)  # [1, H*W, OUT_C]\n",
    "\n",
    "#         # prepare data for decoding\n",
    "#         enc_outs = enc_outs.expand(self.beam_size, -1, -1)\n",
    "#         # [Beam_size, dec_rnn_h]\n",
    "#         dec_states, O_t = self.model.init_decoder(enc_outs)\n",
    "\n",
    "#         # store top k ids (k is less or equal to beam_size)\n",
    "#         # in first decoding step, all they are  start token\n",
    "#         topk_ids = torch.ones(\n",
    "#             self.beam_size, device=self.device).long() * START_TOKEN\n",
    "#         topk_log_probs = torch.Tensor([0.0] + [-1e10] * (self.beam_size - 1))\n",
    "#         topk_log_probs = topk_log_probs.to(self.device)\n",
    "#         seqs = torch.ones(\n",
    "#             self.beam_size, 1, device=self.device).long() * START_TOKEN\n",
    "#         # store complete sequences and corrosponing scores\n",
    "#         complete_seqs = []\n",
    "#         complete_seqs_scores = []\n",
    "#         k = self.beam_size\n",
    "#         vocab_size = len(self._sign2id)\n",
    "#         with torch.no_grad():\n",
    "#             for t in range(self.max_len):\n",
    "#                 dec_states, O_t, logit = self.model.step_decoding(\n",
    "#                     dec_states, O_t, enc_outs, topk_ids.unsqueeze(1))\n",
    "#                 log_probs = torch.log(logit)  # [k, vocab_size]\n",
    "\n",
    "#                 log_probs += topk_log_probs.unsqueeze(1)\n",
    "#                 topk_log_probs, topk_ids = torch.topk(log_probs.view(-1), k)\n",
    "\n",
    "#                 beam_index = topk_ids // vocab_size\n",
    "#                 topk_ids = topk_ids % vocab_size\n",
    "\n",
    "#                 seqs = torch.cat(\n",
    "#                     [seqs.index_select(0, beam_index), topk_ids.unsqueeze(1)],\n",
    "#                     dim=1\n",
    "#                 )\n",
    "\n",
    "#                 complete_inds = [\n",
    "#                     ind for ind, next_word in enumerate(topk_ids)\n",
    "#                     if next_word == END_TOKEN\n",
    "#                 ]\n",
    "#                 if t == (self.max_len-1):  # last_step, end all seqs\n",
    "#                     complete_inds = list(range(len(topk_ids)))\n",
    "\n",
    "#                 incomplete_inds = list(\n",
    "#                     set(range(len(topk_ids))) - set(complete_inds)\n",
    "#                 )\n",
    "#                 if len(complete_inds) > 0:\n",
    "#                     complete_seqs.extend(seqs[complete_inds])\n",
    "#                     complete_seqs_scores.extend(topk_log_probs[complete_inds])\n",
    "#                 k -= len(complete_inds)\n",
    "#                 if k == 0:  # all beam finished\n",
    "#                     break\n",
    "\n",
    "#                 # prepare for next step\n",
    "#                 seqs = seqs[incomplete_inds]\n",
    "#                 topk_ids = topk_ids[incomplete_inds]\n",
    "#                 topk_log_probs = topk_log_probs[incomplete_inds]\n",
    "\n",
    "#                 enc_outs = enc_outs[:k]\n",
    "#                 seleted = beam_index[incomplete_inds]\n",
    "#                 O_t = O_t[seleted]\n",
    "#                 dec_states = (dec_states[0][seleted],\n",
    "#                               dec_states[1][seleted])\n",
    "\n",
    "#         i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "#         seq = complete_seqs[i][1:]\n",
    "#         result = self._idx2formulas(seq.unsqueeze(0))[0]\n",
    "#         return result\n",
    "\n",
    "#     def _batch_beam_search(self, imgs):\n",
    "#         self.model.eval()\n",
    "#         imgs = imgs.to(self.device)\n",
    "#         enc_outs = self.model.encode(imgs)  # [batch_size, H*W, OUT_C]\n",
    "#         # enc_outs = enc_outs.expand(self.beam_size, -1, -1)\n",
    "#         dec_states, O_t = self.model.init_decoder(enc_outs)\n",
    "\n",
    "#         batch_size = imgs.size(0)\n",
    "#         start_predictions = torch.ones(\n",
    "#             batch_size, device=self.device).long() * START_TOKEN\n",
    "#         state = {}\n",
    "#         state['h_t'] = dec_states[0]\n",
    "#         state['c_t'] = dec_states[1]\n",
    "#         state['o_t'] = O_t\n",
    "#         state['enc_outs'] = enc_outs\n",
    "#         all_top_k_predictions, log_probabilities = self._beam_search.search(\n",
    "#             start_predictions, state, self._take_step)\n",
    "\n",
    "#         all_top_predictions = all_top_k_predictions[:, 0, :]\n",
    "#         all_top_predictions = self._idx2formulas(all_top_predictions)\n",
    "#         return all_top_predictions\n",
    "\n",
    "    def _take_step(self, last_predictions, state):\n",
    "        dec_states = (state['h_t'], state['c_t'])\n",
    "        O_t = state['o_t']\n",
    "        enc_outs = state['enc_outs']\n",
    "\n",
    "        last_predictions = last_predictions.unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            dec_states, O_t, logit = self.model.step_decoding(\n",
    "                dec_states, O_t, enc_outs, last_predictions)\n",
    "\n",
    "        # update state\n",
    "        state['h_t'] = dec_states[0]\n",
    "        state['c_t'] = dec_states[1]\n",
    "        state['o_t'] = O_t\n",
    "        return (torch.log(logit), state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "def collate_fn(sign2id, batch):\n",
    "    # filter the pictures that have different weight or height\n",
    "    size = batch[0][0].size()\n",
    "    batch = [img_formula for img_formula in batch\n",
    "             if img_formula[0].size() == size]\n",
    "    # sort by the length of formula\n",
    "    batch.sort(key=lambda img_formula: len(img_formula[1].split()),\n",
    "               reverse=True)\n",
    "\n",
    "    imgs, formulas = zip(*batch)\n",
    "    formulas = [formula.split() for formula in formulas]\n",
    "    # targets for training , begin with START_TOKEN\n",
    "    tgt4training = formulas2tensor(add_start_token(formulas), sign2id)\n",
    "    # targets for calculating loss , end with END_TOKEN\n",
    "    tgt4cal_loss = formulas2tensor(add_end_token(formulas), sign2id)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return imgs, tgt4training, tgt4cal_loss\n",
    "\n",
    "\n",
    "def formulas2tensor(formulas, sign2id):\n",
    "    \"\"\"convert formula to tensor\"\"\"\n",
    "\n",
    "    batch_size = len(formulas)\n",
    "    max_len = len(formulas[0])\n",
    "    tensors = torch.ones(batch_size, max_len, dtype=torch.long) * PAD_TOKEN\n",
    "    for i, formula in enumerate(formulas):\n",
    "        for j, sign in enumerate(formula):\n",
    "            tensors[i][j] = sign2id.get(sign, UNK_TOKEN)\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def add_start_token(formulas):\n",
    "    return [['<s>']+formula for formula in formulas]\n",
    "\n",
    "\n",
    "def add_end_token(formulas):\n",
    "    return [formula+['</s>'] for formula in formulas]\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"count model parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def tile(x, count, dim=0):\n",
    "    \"\"\"\n",
    "    Tiles x on dimension dim count times.\n",
    "    \"\"\"\n",
    "    perm = list(range(len(x.size())))\n",
    "    if dim != 0:\n",
    "        perm[0], perm[dim] = perm[dim], perm[0]\n",
    "        x = x.permute(perm).contiguous()\n",
    "    out_size = list(x.size())\n",
    "    out_size[0] *= count\n",
    "    batch = x.size(0)\n",
    "    x = x.view(batch, -1) \\\n",
    "         .transpose(0, 1) \\\n",
    "         .repeat(count, 1) \\\n",
    "         .transpose(0, 1) \\\n",
    "         .contiguous() \\\n",
    "         .view(*out_size)\n",
    "    if dim != 0:\n",
    "        x = x.permute(perm).contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_formulas(filename):\n",
    "    formulas = dict()\n",
    "    with open(filename) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            formulas[idx] = line.strip()\n",
    "    print(\"Loaded {} formulas from {}\".format(len(formulas), filename))\n",
    "    return formulas\n",
    "\n",
    "\n",
    "def cal_loss(logits, targets):\n",
    "    \"\"\"args:\n",
    "        logits: probability distribution return by model\n",
    "                [B, MAX_LEN, voc_size]\n",
    "        targets: target formulas\n",
    "                [B, MAX_LEN]\n",
    "    \"\"\"\n",
    "    padding = torch.ones_like(targets) * PAD_TOKEN\n",
    "    mask = (targets != padding)\n",
    "\n",
    "    targets = targets.masked_select(mask)\n",
    "    logits = logits.masked_select(\n",
    "        mask.unsqueeze(2).expand(-1, -1, logits.size(2))\n",
    "    ).contiguous().view(-1, logits.size(2))\n",
    "    logits = torch.log(logits)\n",
    "\n",
    "    assert logits.size(0) == targets.size(0)\n",
    "\n",
    "    loss = F.nll_loss(logits, targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_checkpoint(ckpt_dir):\n",
    "    \"\"\"return full path if there is ckpt in ckpt_dir else None\"\"\"\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        raise FileNotFoundError(\"No checkpoint found in {}\".format(ckpt_dir))\n",
    "\n",
    "    ckpts = [f for f in os.listdir(ckpt_dir) if f.startswith('ckpt')]\n",
    "    if not ckpts:\n",
    "        raise FileNotFoundError(\"No checkpoint found in {}\".format(ckpt_dir))\n",
    "\n",
    "    last_ckpt, max_epoch = None, 0\n",
    "    for ckpt in ckpts:\n",
    "        epoch = int(ckpt.split('-')[1])\n",
    "        if epoch > max_epoch:\n",
    "            max_epoch = epoch\n",
    "            last_ckpt = ckpt\n",
    "    full_path = os.path.join(ckpt_dir, last_ckpt)\n",
    "    print(\"Get checkpoint from {} for training\".format(full_path))\n",
    "    return full_path\n",
    "\n",
    "\n",
    "def schedule_sample(prev_logit, prev_tgt, epsilon):\n",
    "    prev_out = torch.argmax(prev_logit, dim=1, keepdim=True)\n",
    "    prev_choices = torch.cat([prev_out, prev_tgt], dim=1)  # [B, 2]\n",
    "    batch_size = prev_choices.size(0)\n",
    "    prob = Bernoulli(torch.tensor([epsilon]*batch_size).unsqueeze(1))\n",
    "    # sampling\n",
    "    sample = prob.sample().long().to(prev_tgt.device)\n",
    "    next_inp = torch.gather(prev_choices, 1, sample)\n",
    "    return next_inp\n",
    "\n",
    "\n",
    "def cal_epsilon(k, step, method):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "        Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\n",
    "        See details in https://arxiv.org/pdf/1506.03099.pdf\n",
    "    \"\"\"\n",
    "    assert method in ['inv_sigmoid', 'exp', 'teacher_forcing']\n",
    "\n",
    "    if method == 'exp':\n",
    "        return k**step\n",
    "    elif method == 'inv_sigmoid':\n",
    "        return k/(k+math.exp(step/k))\n",
    "    else:\n",
    "        return 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, optimizer, model, lr_scheduler,\n",
    "                 train_loader, val_loader, args,\n",
    "                 use_cuda=True, init_epoch=1, last_epoch=15):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.args = args\n",
    "\n",
    "        self.step = 0\n",
    "        self.epoch = init_epoch\n",
    "        self.total_step = (init_epoch-1)*len(train_loader)\n",
    "        self.last_epoch = last_epoch\n",
    "        self.best_val_loss = 1e18\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    def train(self):\n",
    "        mes = \"Epoch {}, step:{}/{} {:.2f}%, Loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "\n",
    "        while self.epoch <= self.last_epoch:\n",
    "            self.model.train()\n",
    "            losses = 0.0\n",
    "            for imgs, tgt4training, tgt4cal_loss in self.train_loader:\n",
    "                step_loss = self.train_step(imgs, tgt4training, tgt4cal_loss)\n",
    "                losses += step_loss\n",
    "\n",
    "                # log message\n",
    "                if self.step % self.args.print_freq == 0:\n",
    "                    avg_loss = losses / self.args.print_freq\n",
    "                    print(mes.format(\n",
    "                        self.epoch, self.step, len(self.train_loader),\n",
    "                        100 * self.step / len(self.train_loader),\n",
    "                        avg_loss,\n",
    "                        2**avg_loss\n",
    "                    ))\n",
    "                    losses = 0.0\n",
    "\n",
    "            # one epoch Finished, calcute val loss\n",
    "            val_loss = self.validate()\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "            self.save_model('ckpt-{}-{:.4f}'.format(self.epoch, val_loss))\n",
    "            self.epoch += 1\n",
    "            self.step = 0\n",
    "\n",
    "    def train_step(self, imgs, tgt4training, tgt4cal_loss):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(self.device)\n",
    "        tgt4training = tgt4training.to(self.device)\n",
    "        tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "        epsilon = cal_epsilon(\n",
    "            self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "        logits = self.model(imgs, tgt4training, epsilon)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = cal_loss(logits, tgt4cal_loss)\n",
    "        self.step += 1\n",
    "        self.total_step += 1\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), self.args.clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        mes = \"Epoch {}, validation average loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "        with torch.no_grad():\n",
    "            for imgs, tgt4training, tgt4cal_loss in self.val_loader:\n",
    "                imgs = imgs.to(self.device)\n",
    "                tgt4training = tgt4training.to(self.device)\n",
    "                tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "\n",
    "                epsilon = cal_epsilon(\n",
    "                    self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "                logits = self.model(imgs, tgt4training, epsilon)\n",
    "                loss = cal_loss(logits, tgt4cal_loss)\n",
    "                val_total_loss += loss\n",
    "            avg_loss = val_total_loss / len(self.val_loader)\n",
    "            print(mes.format(\n",
    "                self.epoch, avg_loss, 2**avg_loss\n",
    "            ))\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_model('best_ckpt')\n",
    "        return avg_loss\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        if not os.path.isdir(self.args.save_dir):\n",
    "            os.makedirs(self.args.save_dir)\n",
    "        save_path = join(self.args.save_dir, model_name+'.pt')\n",
    "        print(\"Saving checkpoint to {}\".format(save_path))\n",
    "\n",
    "        # torch.save(self.model, model_path)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'lr_sche': self.lr_scheduler.state_dict(),\n",
    "            'epoch': self.epoch,\n",
    "            'args': self.args\n",
    "        }, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting distance\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: distance\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16260 sha256=6ab2408c993c83d007cd20db8189cc451cf18801558e35ad52a96fde5403a40e\n",
      "  Stored in directory: /home/tron/.cache/pip/wheels/b2/10/1b/96fca621a1be378e2fe104cfb0d160bb6cdf3d04a3d35266cc\n",
      "Successfully built distance\n",
      "Installing collected packages: distance\n",
      "Successfully installed distance-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import distance\n",
    "\n",
    "\n",
    "def score_files(path_ref, path_hyp):\n",
    "    \"\"\"Loads result from file and score it\n",
    "    Args:\n",
    "        path_ref: (string) formulas of reference\n",
    "        path_hyp: (string) formulas of prediction.\n",
    "    Returns:\n",
    "        scores: (dict)\n",
    "    \"\"\"\n",
    "    # load formulas\n",
    "    formulas_ref = load_formulas(path_ref)\n",
    "    formulas_hyp = load_formulas(path_hyp)\n",
    "\n",
    "    assert len(formulas_ref) == len(formulas_hyp)\n",
    "\n",
    "    # tokenize\n",
    "    refs = [ref.split(' ') for _, ref in formulas_ref.items()]\n",
    "    hyps = [hyp.split(' ') for _, hyp in formulas_hyp.items()]\n",
    "\n",
    "    # score\n",
    "    return {\n",
    "        \"BLEU-4\": bleu_score(refs, hyps)*100,\n",
    "        \"EM\": exact_match_score(refs, hyps)*100,\n",
    "        \"Edit\": edit_distance(refs, hyps)*100\n",
    "    }\n",
    "\n",
    "\n",
    "def exact_match_score(references, hypotheses):\n",
    "    \"\"\"Computes exact match scores.\n",
    "    Args:\n",
    "        references: list of list of tokens (one ref)\n",
    "        hypotheses: list of list of tokens (one hypothesis)\n",
    "    Returns:\n",
    "        exact_match: (float) 1 is perfect\n",
    "    \"\"\"\n",
    "    exact_match = 0\n",
    "    for ref, hypo in zip(references, hypotheses):\n",
    "        if np.array_equal(ref, hypo):\n",
    "            exact_match += 1\n",
    "\n",
    "    return exact_match / float(max(len(hypotheses), 1))\n",
    "\n",
    "\n",
    "def bleu_score(references, hypotheses):\n",
    "    \"\"\"Computes bleu score.\n",
    "    Args:\n",
    "        references: list of list (one hypothesis)\n",
    "        hypotheses: list of list (one hypothesis)\n",
    "    Returns:\n",
    "        BLEU-4 score: (float)\n",
    "    \"\"\"\n",
    "    references = [[ref] for ref in references]  # for corpus_bleu func\n",
    "    BLEU_4 = nltk.translate.bleu_score.corpus_bleu(\n",
    "        references, hypotheses,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25)\n",
    "    )\n",
    "    return BLEU_4\n",
    "\n",
    "\n",
    "def edit_distance(references, hypotheses):\n",
    "    \"\"\"Computes Levenshtein distance between two sequences.\n",
    "    Args:\n",
    "        references: list of list of token (one hypothesis)\n",
    "        hypotheses: list of list of token (one hypothesis)\n",
    "    Returns:\n",
    "        1 - levenshtein distance: (higher is better, 1 is perfect)\n",
    "    \"\"\"\n",
    "    d_leven, len_tot = 0, 0\n",
    "    for ref, hypo in zip(references, hypotheses):\n",
    "        d_leven += distance.levenshtein(ref, hypo)\n",
    "        len_tot += float(max(len(ref), len(hypo)))\n",
    "\n",
    "    return 1. - d_leven / len_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    max_epoch = args.epoches\n",
    "    from_check_point = args.from_check_point\n",
    "    if from_check_point:\n",
    "        checkpoint_path = get_checkpoint(args.save_dir)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args = checkpoint['args']\n",
    "    print(\"Training args:\", args)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # Building vocab\n",
    "    print(\"Load vocab...\")\n",
    "    vocab = load_vocab()\n",
    "\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#     # data loader\n",
    "#     print(\"Construct data loader...\")\n",
    "#     train_loader = DataLoader(\n",
    "#         Im2LatexDataset(args.data_path, 'train', args.max_len),\n",
    "#         batch_size=args.batch_size,\n",
    "#         collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "#         pin_memory=True if use_cuda else False,\n",
    "#         num_workers=4)\n",
    "#     val_loader = DataLoader(\n",
    "#         Im2LatexDataset(args.data_path, 'validate', args.max_len),\n",
    "#         batch_size=args.batch_size,\n",
    "#         collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "#         pin_memory=True if use_cuda else False,\n",
    "#         num_workers=4)\n",
    "\n",
    "    # construct model\n",
    "    print(\"Construct model\")\n",
    "    vocab_size = len(vocab)\n",
    "    model = Image2LatexModel(\n",
    "        vocab_size, args.emb_dim, args.dec_rnn_h,\n",
    "        add_pos_feat=args.add_position_features,\n",
    "        dropout=args.dropout\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"Model Settings:\")\n",
    "    print(model)\n",
    "\n",
    "    # construct optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        \"min\",\n",
    "        factor=args.lr_decay,\n",
    "        patience=args.lr_patience,\n",
    "        verbose=True,\n",
    "        min_lr=args.min_lr)\n",
    "\n",
    "    if from_check_point:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_sche'])\n",
    "        # init trainer from checkpoint\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=epoch, last_epoch=max_epoch)\n",
    "    else:\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=1, last_epoch=args.epoches)\n",
    "    # begin training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "Arguments = namedtuple(\"Arguments\", [\"emb_dim\", \"dec_rnn_h\", \"add_position_features\",\"max_len\",\n",
    "                       \"dropout\", \"cuda\", \"batch_size\", \"epoches\", \n",
    "                       \"lr\", \"min_lr\", \"sample_method\", \"decay_k\",\n",
    "                        \"lr_decay\", \"lr_patience\", \"clip\", \"save_dir\",\n",
    "                       \"print_freq\", \"seed\", \"from_check_point\"]\n",
    "            )\n",
    "\n",
    "args = Arguments(80, 512, True, 150, 0.2, True, 32, 25, 3e-4,\n",
    "                         3e-5, \"teacher_forcing\", 1., 0.5, 3, 2.0,\n",
    "                         os.getcwd(), 100, 2020, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
